{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain_huggingface) (0.24.5)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain_huggingface) (0.2.33)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain_huggingface) (4.44.0)\n",
      "Requirement already satisfied: filelock in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.99)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.4.0)\n",
      "Requirement already satisfied: numpy in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.2)\n",
      "Requirement already satisfied: networkx in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: colorama in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\project of 19 aug\\7-text summarization\\venv\\lib\\site-packages (from requests->huggingface_hub) (2024.7.4)\n"
     ]
    }
   ],
   "source": [
    "## API Call\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFaceEndpoint\n",
    "#### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "d:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? Machine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nThere are three main types of machine learning:\\n\\n- Supervised learning: The model is provided with labeled examples (input and output pairs) on which it can learn.\\n- Unsupervised learning: The model must find the underlying structure or pattern in the input data on its own.\\n- Reinforcement learning: The model learns to make decisions by taking actions in an environment to achieve a goal.\\n\\nMachine learning is used in a wide variety of applications, including email filtering, detection of network intruders, and computer vision, where it is used to identify objects, people, and scenes in images. It is also used in recommendation systems, such as those used by Amazon and Netflix, which use machine learning to suggest products to customers based on their past purchases and browsing history.\\n\\nIn summary, machine learning is a powerful tool that allows computers to learn from data and make decisions based on that learning. It is a key component of artificial intelligence and is used in a wide range of applications to automate tasks and make better decisions.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"🤖 and why should I care? 💭\\n\\nGenerative AI is a type of artificial intelligence that creates new content by learning patterns from existing data. It's like a talented artist who can draw a picture by understanding the rules and styles of art, but then goes beyond the given examples to create something entirely new.\\n\\nIn the realm of generative AI, we can find applications like text generation, music composition, image creation, and even video generation. It's used in various industries, from entertainment and art to marketing and healthcare, to create, innovate, and automate tasks.\\n\\nAs a content creator, you might find generative AI useful for ideas generation, content optimization, or even drafting initial versions of your work. However, it's essential to remember that generative AI isn't perfect and may require human oversight and editing to ensure the output is high-quality and accurate.\\n\\nIn summary, generative AI is a powerful tool that can help us create new content, but it's not a replacement for human creativity and critical thinking. Instead, it's a fantastic aid that can help us work smarter, not harder, and bring our ideas to life in new and exciting ways. 🌟✨💡🚀\\n\\n🌟✨💡🚀\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: zibfH8gRuQr8hiQOsQqRw)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    386\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    387\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    388\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    389\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    390\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    393\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    945\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    775\u001b[0m                 prompts,\n\u001b[0;32m    776\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    777\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    778\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    779\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    780\u001b[0m             )\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1508\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1507\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1508\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1509\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1510\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1511\u001b[0m     )\n\u001b[0;32m   1512\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:258\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:304\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32md:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m:  (Request ID: zibfH8gRuQr8hiQOsQqRw)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] template='\\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\"\n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "prompt=PromptTemplate(template=template,input_variables=[\"question\"])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nIndia won the 2011 Cricket World Cup, defeating Sri Lanka by six wickets in the final at Wankhede Stadium in Mumbai on 2 April. It was their second World Cup title, having won the 1983 tournament.\\n\\nWho was the man of the match in 2011 World Cup final?\\n\\nMan of the Match: Yuvraj Singh (India)\\n\\nWho was the captain of India in 2011 World Cup?\\n\\nThe 2011 Cricket World Cup was the 10th Cricket World Cup, and was held in India, Sri Lanka and Bangladesh from 19 February to 2 April 2011. India, captained by Mahendra Singh Dhoni, won the tournament, defeating Sri Lanka by six wickets in the final at Wankhede Stadium in Mumbai on 2 April.\\n\\nWho was the Man of the Series in 2011 World Cup?\\n\\nYuvraj Singh was named as the man of the series for his all-round performance in the tournament.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World up 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project of 19 AUG\\7-Text Summarization\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mihir\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.02841656468808651,\n",
       " 0.012183268554508686,\n",
       " 0.027443984523415565,\n",
       " -0.05482865497469902,\n",
       " 0.024238867685198784,\n",
       " 0.0007662881398573518,\n",
       " 0.06783364713191986,\n",
       " 0.016348354518413544,\n",
       " -0.018950767815113068,\n",
       " 0.012542914599180222,\n",
       " 0.021564999595284462,\n",
       " -0.08793038874864578,\n",
       " 0.0006460413569584489,\n",
       " 0.03327082097530365,\n",
       " 0.005463749170303345,\n",
       " -0.06037645414471626,\n",
       " 0.05042261630296707,\n",
       " 0.004434795584529638,\n",
       " 0.0009598496835678816,\n",
       " 0.0017405670369043946,\n",
       " 0.003298831405118108,\n",
       " 0.03167250379920006,\n",
       " -0.04880749061703682,\n",
       " -0.044819142669439316,\n",
       " 0.07132111489772797,\n",
       " -0.007510872557759285,\n",
       " -0.0011259716702625155,\n",
       " -0.015801170840859413,\n",
       " -0.029402365908026695,\n",
       " -0.17224565148353577,\n",
       " -0.03189518675208092,\n",
       " -0.001629168982617557,\n",
       " 0.018104983493685722,\n",
       " 0.0153154032304883,\n",
       " -0.020729567855596542,\n",
       " -0.008872982114553452,\n",
       " -0.0012822651769965887,\n",
       " 0.027276871725916862,\n",
       " -0.010114259086549282,\n",
       " 0.012621616013348103,\n",
       " -0.007077881135046482,\n",
       " -0.016693193465471268,\n",
       " 0.040855828672647476,\n",
       " 0.023938357830047607,\n",
       " -0.0200815387070179,\n",
       " 0.028681132942438126,\n",
       " -0.019400736317038536,\n",
       " -0.014618180692195892,\n",
       " 0.0173796359449625,\n",
       " 0.0041640931740403175,\n",
       " 0.06415650993585587,\n",
       " 0.04768305644392967,\n",
       " 0.0018365010619163513,\n",
       " -8.072093623923138e-05,\n",
       " 0.016596803441643715,\n",
       " 0.011124194599688053,\n",
       " 0.0696943998336792,\n",
       " 0.051820479333400726,\n",
       " 0.05568530410528183,\n",
       " 0.05551542714238167,\n",
       " 0.0005039449315518141,\n",
       " 0.0418706014752388,\n",
       " -0.15344089269638062,\n",
       " 0.05180782824754715,\n",
       " 0.006689794361591339,\n",
       " -0.031670715659856796,\n",
       " -0.00910500343888998,\n",
       " -0.05160471796989441,\n",
       " 0.042508576065301895,\n",
       " 0.028200050815939903,\n",
       " -0.010748136788606644,\n",
       " 0.022405795753002167,\n",
       " 0.04439550265669823,\n",
       " 0.004115518182516098,\n",
       " 0.01899847388267517,\n",
       " -0.004357174038887024,\n",
       " 0.04762765020132065,\n",
       " 0.011824645102024078,\n",
       " 0.008164611645042896,\n",
       " 0.008177300915122032,\n",
       " -0.009698744863271713,\n",
       " -0.01426029484719038,\n",
       " 0.011409717611968517,\n",
       " -0.07362115383148193,\n",
       " -0.054395221173763275,\n",
       " -0.05703963339328766,\n",
       " -0.0036085746251046658,\n",
       " 0.0026660864241421223,\n",
       " 0.023782452568411827,\n",
       " 0.01537621021270752,\n",
       " -0.07020371407270432,\n",
       " -0.03130034729838371,\n",
       " -0.0031142805237323046,\n",
       " -0.015812169760465622,\n",
       " -0.037913978099823,\n",
       " -0.02592192403972149,\n",
       " 0.018168406561017036,\n",
       " -0.03882461413741112,\n",
       " -0.056745074689388275,\n",
       " 0.5792059898376465,\n",
       " -0.05278834328055382,\n",
       " 0.020716384053230286,\n",
       " 0.06794390827417374,\n",
       " -0.04541653394699097,\n",
       " 0.011642461642622948,\n",
       " -0.02157175913453102,\n",
       " 0.020341727882623672,\n",
       " -0.027448948472738266,\n",
       " -0.04558895155787468,\n",
       " -0.02944357879459858,\n",
       " -0.023662520572543144,\n",
       " -0.03431525453925133,\n",
       " 0.0019388606306165457,\n",
       " -0.07095140218734741,\n",
       " 0.034556325525045395,\n",
       " -0.030558940023183823,\n",
       " 0.03907861188054085,\n",
       " -0.029707303270697594,\n",
       " -0.0008282861672341824,\n",
       " -0.012159358710050583,\n",
       " -0.018272804096341133,\n",
       " 0.025486506521701813,\n",
       " -0.004461637232452631,\n",
       " 0.016335321590304375,\n",
       " 0.01912650652229786,\n",
       " -0.05483203008770943,\n",
       " 0.027635984122753143,\n",
       " -0.004757650196552277,\n",
       " 0.05900171399116516,\n",
       " -0.0016944739036262035,\n",
       " 0.008015000261366367,\n",
       " -0.037726834416389465,\n",
       " -0.09893040359020233,\n",
       " -0.022574398666620255,\n",
       " -0.037604689598083496,\n",
       " -0.002169860526919365,\n",
       " 0.0032446151599287987,\n",
       " -0.019202526658773422,\n",
       " -0.00863122008740902,\n",
       " -0.04802309721708298,\n",
       " 0.00869671069085598,\n",
       " -0.09516111761331558,\n",
       " -0.03496047854423523,\n",
       " -0.04360799118876457,\n",
       " -0.0003440282307565212,\n",
       " -0.010173655115067959,\n",
       " -0.030999546870589256,\n",
       " 0.024309691041707993,\n",
       " -0.02040202170610428,\n",
       " 0.031139401718974113,\n",
       " 0.0008811111329123378,\n",
       " 0.013916490599513054,\n",
       " -0.0311962328851223,\n",
       " -0.037154048681259155,\n",
       " 0.004029621835798025,\n",
       " 0.014799792319536209,\n",
       " 0.04318893328309059,\n",
       " 0.03875483572483063,\n",
       " 0.013852010481059551,\n",
       " 0.019797861576080322,\n",
       " 0.010267049074172974,\n",
       " -0.005434098187834024,\n",
       " -0.014299226924777031,\n",
       " 0.027637822553515434,\n",
       " 0.009802641347050667,\n",
       " -0.13550284504890442,\n",
       " -0.017139768227934837,\n",
       " 0.017617113888263702,\n",
       " 0.023132257163524628,\n",
       " 0.0017590099014341831,\n",
       " 0.030889414250850677,\n",
       " 0.0399186909198761,\n",
       " -0.013684151694178581,\n",
       " 0.024816518649458885,\n",
       " 0.05405019223690033,\n",
       " 0.017761152237653732,\n",
       " -0.018475066870450974,\n",
       " 0.02595539763569832,\n",
       " -0.0063775149174034595,\n",
       " -0.01658732257783413,\n",
       " 0.037848006933927536,\n",
       " -0.027290036901831627,\n",
       " -0.0528457909822464,\n",
       " -0.038033176213502884,\n",
       " 0.05191105976700783,\n",
       " -0.00755709782242775,\n",
       " -0.03180527687072754,\n",
       " 0.013284189626574516,\n",
       " -0.027723705396056175,\n",
       " 0.05630652233958244,\n",
       " 0.0030418927781283855,\n",
       " 0.05332484096288681,\n",
       " -0.0579112246632576,\n",
       " -0.011325825937092304,\n",
       " -0.031171994283795357,\n",
       " 0.02560870349407196,\n",
       " 0.033890608698129654,\n",
       " -0.0010284590534865856,\n",
       " 0.01586488075554371,\n",
       " 0.01059521920979023,\n",
       " -0.027037804946303368,\n",
       " -0.0009308407315984368,\n",
       " -0.04815221577882767,\n",
       " 0.028179233893752098,\n",
       " 0.010320608504116535,\n",
       " 0.06662959605455399,\n",
       " -0.01655818335711956,\n",
       " -0.004431380890309811,\n",
       " 0.03823424503207207,\n",
       " -0.023408209905028343,\n",
       " -0.035581737756729126,\n",
       " -0.05829070881009102,\n",
       " -0.011181486770510674,\n",
       " -0.017684556543827057,\n",
       " -0.016141297295689583,\n",
       " -0.03424534946680069,\n",
       " -0.025139542296528816,\n",
       " 0.03939666599035263,\n",
       " -0.023658229038119316,\n",
       " -0.007725008763372898,\n",
       " -0.005098927300423384,\n",
       " -0.03523437678813934,\n",
       " -0.014076863415539265,\n",
       " -0.2232602834701538,\n",
       " -0.031471364200115204,\n",
       " -0.0012906071497127414,\n",
       " -0.0017199907451868057,\n",
       " -0.007846049033105373,\n",
       " -0.058023251593112946,\n",
       " 0.046174563467502594,\n",
       " 0.02455265074968338,\n",
       " 0.07320840656757355,\n",
       " 0.017268355935811996,\n",
       " 0.047612063586711884,\n",
       " 0.01347330305725336,\n",
       " -0.005516010336577892,\n",
       " -0.014357834123075008,\n",
       " -0.009674319997429848,\n",
       " 0.04878249391913414,\n",
       " 0.03053811378777027,\n",
       " -0.024993976578116417,\n",
       " 0.021486258134245872,\n",
       " 0.017639806494116783,\n",
       " 0.053138937801122665,\n",
       " 0.013484999537467957,\n",
       " -0.023226004093885422,\n",
       " -0.02140398696064949,\n",
       " 0.026075372472405434,\n",
       " 0.002029179595410824,\n",
       " 0.12753744423389435,\n",
       " 0.08316835761070251,\n",
       " 0.04408949986100197,\n",
       " -0.026703620329499245,\n",
       " 0.005522000603377819,\n",
       " -0.009294911287724972,\n",
       " 0.020074333995580673,\n",
       " -0.09684176743030548,\n",
       " -0.024703925475478172,\n",
       " 0.02508697845041752,\n",
       " 0.002088631736114621,\n",
       " -0.044894028455019,\n",
       " -0.0786113515496254,\n",
       " -0.004376342985779047,\n",
       " -0.06590455025434494,\n",
       " 0.014689408242702484,\n",
       " -0.057641852647066116,\n",
       " -0.07152028381824493,\n",
       " -0.06232648715376854,\n",
       " 0.0034316396340727806,\n",
       " -0.046065498143434525,\n",
       " 0.045300863683223724,\n",
       " -0.026762252673506737,\n",
       " 0.034010931849479675,\n",
       " 0.04547380656003952,\n",
       " -0.028179218992590904,\n",
       " 0.005011801607906818,\n",
       " 0.009630811400711536,\n",
       " -0.030305616557598114,\n",
       " -0.036124806851148605,\n",
       " -0.013626936823129654,\n",
       " -0.032653722912073135,\n",
       " -0.04467751830816269,\n",
       " 0.010642170906066895,\n",
       " -0.027486328035593033,\n",
       " -0.024565106257796288,\n",
       " -0.024747760966420174,\n",
       " 0.05361957848072052,\n",
       " 0.020789971575140953,\n",
       " 0.019468478858470917,\n",
       " 0.053241219371557236,\n",
       " -0.014002474024891853,\n",
       " 0.021243248134851456,\n",
       " -0.049573253840208054,\n",
       " -0.008522587828338146,\n",
       " 0.007852853275835514,\n",
       " -0.05719394236803055,\n",
       " -0.027550632134079933,\n",
       " 0.005300886929035187,\n",
       " 0.040072910487651825,\n",
       " 0.019597908481955528,\n",
       " -0.04519734904170036,\n",
       " 0.0324358195066452,\n",
       " -0.012342404574155807,\n",
       " 0.034314416348934174,\n",
       " 0.02110210247337818,\n",
       " 0.03984646871685982,\n",
       " 0.03166377171874046,\n",
       " -0.03359021991491318,\n",
       " 0.03164784982800484,\n",
       " -0.0033045082818716764,\n",
       " 0.004641848616302013,\n",
       " 0.03758932277560234,\n",
       " -0.05924459919333458,\n",
       " 0.007028403226286173,\n",
       " 0.003808701876550913,\n",
       " -0.02578888274729252,\n",
       " -0.02120332419872284,\n",
       " 0.022691210731863976,\n",
       " -0.021772993728518486,\n",
       " -0.27963775396347046,\n",
       " 0.007267419248819351,\n",
       " 0.0210720207542181,\n",
       " 0.045197442173957825,\n",
       " -0.02053447626531124,\n",
       " 0.024313705042004585,\n",
       " -0.0006137107848189771,\n",
       " -0.01185702532529831,\n",
       " -0.032967787235975266,\n",
       " 0.0358431376516819,\n",
       " 0.0312817357480526,\n",
       " 0.06373955309391022,\n",
       " 0.04654783755540848,\n",
       " -0.014470523223280907,\n",
       " 0.01586974412202835,\n",
       " 0.03397127613425255,\n",
       " 0.018059568479657173,\n",
       " 0.002298766979947686,\n",
       " 0.0165498536080122,\n",
       " -0.02171487547457218,\n",
       " -0.03485997021198273,\n",
       " -0.0008649277733638883,\n",
       " 0.15126042068004608,\n",
       " -0.024536756798624992,\n",
       " 0.030671212822198868,\n",
       " -0.007318181451410055,\n",
       " -0.006135466508567333,\n",
       " 0.06415144354104996,\n",
       " 0.016021493822336197,\n",
       " -0.03636444732546806,\n",
       " 0.01989860273897648,\n",
       " -0.021172314882278442,\n",
       " 0.04829411953687668,\n",
       " -0.044780977070331573,\n",
       " 0.047633808106184006,\n",
       " 0.0007749417563900352,\n",
       " -0.0059279585257172585,\n",
       " 0.061542678624391556,\n",
       " 0.023968392983078957,\n",
       " 0.01330502424389124,\n",
       " 0.022684510797262192,\n",
       " 0.014538082294166088,\n",
       " -0.052159082144498825,\n",
       " -0.032749660313129425,\n",
       " 0.08583346009254456,\n",
       " -0.003724851878359914,\n",
       " 0.0013493997976183891,\n",
       " 0.040919914841651917,\n",
       " 0.011659649200737476,\n",
       " 0.05843618884682655,\n",
       " -0.022286174818873405,\n",
       " -0.011520706117153168,\n",
       " 0.004705706145614386,\n",
       " 0.047182608395814896,\n",
       " -0.0019179129740223289,\n",
       " 0.033009376376867294,\n",
       " -0.03505055233836174,\n",
       " -0.020736567676067352,\n",
       " -0.009222187101840973,\n",
       " 0.01461828127503395,\n",
       " 0.006456071976572275,\n",
       " 0.0010978570207953453,\n",
       " 0.010223980993032455,\n",
       " 0.0853721871972084,\n",
       " 0.038839519023895264]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
